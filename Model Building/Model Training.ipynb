{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61836da6",
   "metadata": {},
   "source": [
    "# A Gesture-Based Tool For Sterile Browsing Of Radiology Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835453f7",
   "metadata": {},
   "source": [
    "In this project we have used Convolutional Neural Network to first train the model on the images of different hand gestures, like showing numbers with fingers as 0,1,2,3,4,5. Then we made a web portal using Flask where user can input any image on which he wants to perform the operations. After uploading the image, our portal uses the integrated webcam to capture the video frame using OpenCV. The gesture captured in the video frame is compared with the Pre-trained model and the gesture is identified. If the prediction is 0 - then images is converted into rectangle, 1 - image is Resized into (200,200), 2 - image is rotated by -45à¥°, 3 - image is blurred , 4 - image is Resized into (400,400) , 5 - image is converted into grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e31015",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b67adb",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eaa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library helps add support for large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "#open source used for both ML and DL for computation\n",
    "import tensorflow as tf\n",
    "#it is a plain stack of layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "#Dense layer is the regular deeply connected neural network layer\n",
    "from tensorflow.keras.layers import Dense,Flatten, Dropout\n",
    "#Faltten-used fot flattening the input or change the dimension, MaxPooling2D-for downsampling the image for Convolutional layer\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D \n",
    "#Its used for different augmentation of the image \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2a72f",
   "metadata": {},
   "source": [
    "## Augmenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9562f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameter for Image Data agumentation to the traing data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "#Image Data agumentation to the testing data\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b445d2",
   "metadata": {},
   "source": [
    "## Loading our data and performing data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604e213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 594 images belonging to 6 classes.\n",
      "Found 30 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "#performing data agumentation to train data\n",
    "x_train = train_datagen.flow_from_directory(r'C:\\Users\\chandu\\Desktop\\IBM_Project\\Dataset\\train',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=3,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical')\n",
    "#performing data agumentation to test data\n",
    "x_test = test_datagen.flow_from_directory(r'C:\\Users\\chandu\\Desktop\\IBM_Project\\Dataset\\test',\n",
    "                                          target_size=(64, 64),\n",
    "                                          batch_size=3,\n",
    "                                          color_mode='grayscale',\n",
    "                                          class_mode='categorical') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9cbc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)#checking the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eab266",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580b26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f46fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolution layer and pooling\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02184ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second convolution layer and pooling\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6beb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the layers i.e. input layer\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f0fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fully connected layer, i.e. Hidden Layer\n",
    "model.add(Dense(units=512 , activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ec1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax for categorical analysis, Output Layer\n",
    "model.add(Dense(units=6, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f5df3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,224,422\n",
      "Trainable params: 3,224,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()#summary of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbc71d",
   "metadata": {},
   "source": [
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94fb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "# categorical_crossentropy for more than 2\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56e6b3",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f2acb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chandu\\AppData\\Local\\Temp/ipykernel_16664/804983804.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(x_train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "198/198 [==============================] - 15s 64ms/step - loss: 1.3510 - accuracy: 0.4663 - val_loss: 0.6526 - val_accuracy: 0.7333\n",
      "Epoch 2/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.5420 - accuracy: 0.7862 - val_loss: 0.5013 - val_accuracy: 0.8333\n",
      "Epoch 3/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.4282 - accuracy: 0.8350 - val_loss: 0.3105 - val_accuracy: 0.8667\n",
      "Epoch 4/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.3142 - accuracy: 0.8788 - val_loss: 0.3412 - val_accuracy: 0.9333\n",
      "Epoch 5/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.1804 - accuracy: 0.9411 - val_loss: 0.1762 - val_accuracy: 0.9667\n",
      "Epoch 6/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.1319 - accuracy: 0.9512 - val_loss: 0.2698 - val_accuracy: 0.9000\n",
      "Epoch 7/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.1519 - accuracy: 0.9411 - val_loss: 0.3620 - val_accuracy: 0.9333\n",
      "Epoch 8/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.1084 - accuracy: 0.9545 - val_loss: 0.3186 - val_accuracy: 0.9000\n",
      "Epoch 9/25\n",
      "198/198 [==============================] - 6s 33ms/step - loss: 0.1095 - accuracy: 0.9630 - val_loss: 0.1504 - val_accuracy: 0.9667\n",
      "Epoch 10/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.0509 - accuracy: 0.9815 - val_loss: 0.2306 - val_accuracy: 0.9000\n",
      "Epoch 11/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.1230 - accuracy: 0.9630 - val_loss: 0.1397 - val_accuracy: 0.9333\n",
      "Epoch 12/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.0989 - accuracy: 0.9646 - val_loss: 0.2156 - val_accuracy: 0.9333\n",
      "Epoch 13/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.0348 - accuracy: 0.9899 - val_loss: 0.2288 - val_accuracy: 0.9667\n",
      "Epoch 14/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.0788 - accuracy: 0.9663 - val_loss: 0.1042 - val_accuracy: 0.9667\n",
      "Epoch 15/25\n",
      "198/198 [==============================] - 6s 33ms/step - loss: 0.0684 - accuracy: 0.9798 - val_loss: 0.1406 - val_accuracy: 0.9667\n",
      "Epoch 16/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.0505 - accuracy: 0.9848 - val_loss: 0.0880 - val_accuracy: 0.9667\n",
      "Epoch 17/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.1924 - val_accuracy: 0.9333\n",
      "Epoch 18/25\n",
      "198/198 [==============================] - 7s 33ms/step - loss: 0.0261 - accuracy: 0.9933 - val_loss: 0.0980 - val_accuracy: 0.9667\n",
      "Epoch 19/25\n",
      "198/198 [==============================] - 7s 35ms/step - loss: 0.0499 - accuracy: 0.9781 - val_loss: 0.2703 - val_accuracy: 0.9333\n",
      "Epoch 20/25\n",
      "198/198 [==============================] - 7s 37ms/step - loss: 0.0360 - accuracy: 0.9882 - val_loss: 0.2653 - val_accuracy: 0.9667\n",
      "Epoch 21/25\n",
      "198/198 [==============================] - 6s 32ms/step - loss: 0.0390 - accuracy: 0.9865 - val_loss: 0.1269 - val_accuracy: 0.9667\n",
      "Epoch 22/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.0360 - accuracy: 0.9832 - val_loss: 0.2363 - val_accuracy: 0.9667\n",
      "Epoch 23/25\n",
      "198/198 [==============================] - 6s 33ms/step - loss: 0.0605 - accuracy: 0.9848 - val_loss: 0.3152 - val_accuracy: 0.9000\n",
      "Epoch 24/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.0243 - accuracy: 0.9949 - val_loss: 0.2422 - val_accuracy: 0.9333\n",
      "Epoch 25/25\n",
      "198/198 [==============================] - 7s 34ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.1903 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ed32c62430>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will generate packets of train and test data for training\n",
    "model.fit_generator(x_train,\n",
    "                    steps_per_epoch = 594/3 , \n",
    "                    epochs = 25, \n",
    "                    validation_data = x_test,\n",
    "                    validation_steps = 30/3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731067cd",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103e5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('gesture.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd140a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model-bw.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9056784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
